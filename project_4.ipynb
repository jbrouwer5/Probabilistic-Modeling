{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Probabilistic Modeling\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This project focuses on probabilistic models and how to fit them.\n",
    "\n",
    "By the end of this project, you will have implemented the underlying models and fitting procedures:\n",
    "- Naive Bayes for text classification\n",
    "- Mixture of Bernoullis (fit via Expectation-Maximization) for generative image modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable\n",
    "* Project report/writeup: A `project4_report_lastname.pdf` file with corresponding plots and results for the project. Follow the `Project 4 - Report (Individual Submission)` link on Gradescope to upload this file. Also, please put your name on the report so that the grader can find your code easier on the Gradescope.\n",
    "\n",
    "    The project report should include a brief justification of your solution at a high-level, e.g., using any relevant explanations, equations, or pictures that help to explain your solution. You should also describe what your code does, e.g. using a couple of sentences per function to describe your code structure. The objective is to make the report self-contained for grading.\n",
    "\n",
    "    * To be more specific, in addition to textual descriptions and explanations for each sections, you should include images/screenshots/code blocks of:\n",
    "        1. Any code snippets you implemented\n",
    "        2. Plots affected by your implementation, e.g., scatter plots with decision boundaries or loss curves.\n",
    "        3. Output results of the questions, e.g., the best lambda from cross validation.\n",
    "        4. Please do NOT include debugging messages.\n",
    "\n",
    "* Source code: A `project4_src_lastname1[_lastname2].ipynb` (or `.zip`) file with a working copy of your solutions compiled in a Jupyter notebook. Follow the `Project 4 - Source Code (Group Submission)` link to upload this file.\n",
    "    * You are asked to complete code snippets in the following two formats:\n",
    "        1. Between comments of `Your code starts here` and `End of Your Code`.\n",
    "        2. Inline comments with `Your code here`.\n",
    "\n",
    "\n",
    "## Logistics\n",
    "\n",
    "* You can work in groups of 1-2 students for each course project, and it's your responsibility to find a group (e.g. use Ed Discussion). \n",
    "* Every member of a group must complete and submit the project report/writeup individually. While the source code can be the same for all group members, the project report needs to be written independently by each person and, thus, should differ among team member and students more generally.\n",
    "* One one group member need to submit the source code. If you submit as a group, make sure to include your teammate in the group submission. Instructions for team submission can be found [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "* Grades will be provided based on the individual project report. The source code submission will not be graded, but the teaching staff may check the source files if they see the need for reproducing your results when going through your project report. \n",
    "* Failure to submit the source code will lead to a deduction of points from your total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4A: Naïve Bayes (30pt)\n",
    "\n",
    "In this assignment, you will be implementing the Naïve Bayes classifier to perform text classification. The Naïve Bayes classifier is a probabilistic machine learning model that is based on Bayes' theorem. It is particularly useful for classification tasks involving high-dimensional data such as text classification.\n",
    "\n",
    "### Background\n",
    "\n",
    "#### Bayes' Theorem\n",
    "\n",
    "Bayes' theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For a class variable $Y$ and a dependent feature vector $X_1$ through $X_n$, Bayes' theorem is stated as:\n",
    "\n",
    "$$P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}$$\n",
    "\n",
    "In the context of text classification, $Y$ represents a particular class (e.g., spam or not spam), and $X$ represents a text document. The theorem is applied to calculate the probability of a document belonging to a class, given the observed words in the document.\n",
    "\n",
    "#### Naïve Bayes\n",
    "\n",
    "The Naïve Bayes classifier simplifies the calculation by assuming that the features (words in the case of text classification) are conditionally independent given the class. This assumption allows us to factorize the joint probability $P(X|Y)$ into a product of *individual* probabilities:\n",
    "\n",
    "$$P(X|Y) = \\prod_{i=1}^{n} P(X_i|Y)$$\n",
    "\n",
    "where $P(X_i|Y)$ is the probability of word $i$ occurring in a document of class $Y$.\n",
    "\n",
    "This assumption dramatically simplifies the fitting procedure. Naturally, this model is \"naive\" because the features are typically not so simple as to be independent of each other, even conditioned on the class label.\n",
    "\n",
    "#### Using the class-conditional density for classification\n",
    "\n",
    "Suppose we have some class $c$ and a feature vector $x \\in \\mathbb{R}^d$. We directly model the density of data as\n",
    "$$ P_{\\theta}(x | y=c) = \\prod_{i=1}^d P_{\\theta_{ic}}(x_i|y=c) $$\n",
    "where the $\\theta_{ic}$ parameters describe the density of the $i$-th feature in class $c$. For example, if the features are binary, we might use Bernoulli distributions for each feature in each class.\n",
    "\n",
    "In our case, we will be using the Gaussian distribution to model the data density $P(X|Y)$.\n",
    "As such, our probabilities $P_{\\theta_{ic}}(x_i | y=c)$ will come from the Gaussian density.\n",
    "\n",
    "Once the parameters have been estimated on some training set, we can classify new data by computing the posterior over the class labels (our model's output of probabilities for each class given some input test data point $x$) using Bayes' theorem:\n",
    "\n",
    "$$ P_{\\theta}(y=c | x) = \\frac{ P_{\\theta}(y=c) \\prod_{i=1}^d P_{\\theta}(x_i | y=c) }{ \\sum_{c'} P_{\\theta}(y=c) \\prod_{i=1}^d P_{\\theta}(x_i | y=c') } $$\n",
    "\n",
    "For further reading on Naive Bayes, see Section 9.3 in Murphy's Probabilistic Machine Learning (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_dataset():\n",
    "    # Generate a dataset for classification\n",
    "    X, y = make_classification(n_samples=1250, n_features=2, n_redundant=0, \n",
    "                               n_informative=2, random_state=1, \n",
    "                               n_clusters_per_class=1)\n",
    "    \n",
    "    # Add noise to make the dataset linearly inseparable\n",
    "    rng = np.random.RandomState(2)\n",
    "    X += 3 * rng.uniform(size=X.shape)\n",
    "    \n",
    "    # Scale the features\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = create_dataset()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.1713901   1.23562216]\n",
      " [-0.98384316  0.17687108]\n",
      " [ 0.37328084 -0.19142968]\n",
      " [-0.93230629  0.07501438]\n",
      " [-0.29132123 -0.99651735]\n",
      " [-0.06834361  0.99915963]\n",
      " [-0.32883917 -0.88981448]\n",
      " [ 1.05911565 -0.8069675 ]\n",
      " [-0.65279666 -2.0608757 ]\n",
      " [-0.57782301 -0.88249444]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter code\n",
    "\n",
    "This code has been provided for you; there is no need to modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gaussian_pdf(x, mean, var):\n",
    "    \"\"\"Calculate the Gaussian probability density function.\"\"\"\n",
    "    eps = 1e-4  # to prevent division by zero\n",
    "    coeff = 1.0 / np.sqrt(2.0 * np.pi * var + eps)\n",
    "    exponent = np.exp(-(np.power(x - mean, 2) / (2 * var + eps)))\n",
    "    return coeff * exponent\n",
    "\n",
    "def evaluate_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Array of true labels\n",
    "    - y_pred: Array of predicted labels\n",
    "    \n",
    "    Returns:\n",
    "    - Accuracy: The proportion of correctly predicted labels\n",
    "    \"\"\"\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    total_predictions = len(y_true)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4A-I: Calculate Prior Probabilities\n",
    "\n",
    "Note that in the $P_{\\theta}(y=c|x)$ expression we have from earlier, we have the prior class probabilities $P_{\\theta}(y=c)$. You can think of this as the empirical probability of seeing class $c$ in the training data. We can calculate these probabilities by counting the occurrences of each class in the training data and dividing by the total number of samples.\n",
    "\n",
    "Implement the function `calculate_class_priors(y_train)` to calculate and return the prior probabilities \\(P(Y)\\) of each class in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the prior probabilities of each class\n",
    "def calculate_class_priors(y_train):\n",
    "    \"\"\"\n",
    "    Calculates the prior probabilities P(Y) for each class in the training set.\n",
    "\n",
    "    For example, if we have a vector y_train = [0, 1, 0, 0, 1, 0]\n",
    "    composed of two classes 0 and 1, the prior probabilities of each class\n",
    "    that you would return are:\n",
    "    {\n",
    "        0: 0.666,\n",
    "        1: 0.333,\n",
    "    }\n",
    "    \n",
    "    Parameters:\n",
    "    - y_train: Array of training labels\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of prior probabilities for each class\n",
    "    \"\"\"\n",
    "    priors = {}\n",
    "    ### Your code starts here\n",
    "    for y in y_train:\n",
    "        if y not in priors:\n",
    "            priors[y] = 0\n",
    "\n",
    "        priors[y] += 1\n",
    "    \n",
    "    for key in priors.keys():\n",
    "        priors[key] /= len(y_train)\n",
    "\n",
    "\n",
    "    ### End of your code\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.503, 1: 0.497}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_class_priors(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4A-II: Calculate class-conditional density parameters\n",
    "\n",
    "Implement the function `calculate_gaussian_density_params(features, labels)` to calculate the parameters of the class-conditional data density \\(P(X|Y)\\) of observing each feature given each class. Since we're using Gaussians to model the distribution of each feature, you simply need to compute the mean and variance of each feature, conditional on a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the parameters for the class-conditional Gaussian\n",
    "# density of each feature\n",
    "def calculate_gaussian_density_params(features, labels):\n",
    "    \"\"\"\n",
    "    Calculates the likelihood P(X|Y) for each feature given a class.\n",
    "\n",
    "    For example, if we had one feature X_1 and a target Y \\in {0, 1},\n",
    "    we would return:\n",
    "    {\n",
    "        0: [(mean(X_1 | Y=0), var(X_1 | Y=0))],\n",
    "        1: [(mean(X_1 | Y=1), var(X_1 | Y=1))],\n",
    "    }\n",
    "    \n",
    "    Parameters:\n",
    "    - features: Array of features in the training set\n",
    "    - labels: Array of labels corresponding to the features\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of Gaussian density parameters for each feature given each class\n",
    "    \"\"\"\n",
    "    likelihood = {}\n",
    "    ### Your code starts here\n",
    "    for i, f in enumerate(features):\n",
    "        if labels[i] not in likelihood:\n",
    "            likelihood[labels[i]] = [np.array([]) for _ in range(len(features[0]))] \n",
    "        \n",
    "        for j in range(len(f)):\n",
    "            likelihood[labels[i]][j] = np.append(likelihood[labels[i]][j],f[j])\n",
    "    \n",
    "    \n",
    "    for label in likelihood.keys():\n",
    "        for k in range(len(features[0])):\n",
    "            likelihood[label][k] = [np.mean(likelihood[label][k]), np.var(likelihood[label][k])]\n",
    "\n",
    "    ### End of your code\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [[-0.006662567002389797, 1.1635813391635368],\n",
       "  [0.6468447391288673, 0.6059678017903248]],\n",
       " 1: [[0.01161746002515337, 0.8729741552459485],\n",
       "  [-0.6182911080693506, 0.6136216811718437]]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_gaussian_density_params(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4A-III: Implement the Classifier\n",
    "\n",
    "Implement the `naive_bayes_classifier(X_train, y_train, X_test)` function. This function should use the priors and likelihoods calculated in the previous tasks to classify each sample in the test set. Apply Bayes' theorem to compute the posterior probability for each class given the sample and predict the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier function\n",
    "import math\n",
    "from numpy import log10, sign\n",
    "\n",
    "\n",
    "def naive_bayes_classifier(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Classifies each sample in the test set based on the Naive Bayes algorithm.\n",
    "\n",
    "    Steps:\n",
    "    - Compute class prior probabilities using the training set labels.\n",
    "    - Compute class-conditional density parameters from training data.\n",
    "    - For each sample x in the test set:\n",
    "        - Use the density params (mu_i, var_i) learned from training data to compute\n",
    "          the log-likelihood of observing feature x_i.\n",
    "        - Apply Bayes theorem to compute posterior class probabilities P(y|x)\n",
    "          from the feature log-likelihood and prior log-likelihood.\n",
    "          You will use log-likelihood here to avoid underflow.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training set features\n",
    "    - y_train: Training set labels\n",
    "    - X_test: Test set features\n",
    "    \n",
    "    Returns:\n",
    "    - Predicted classes for the test set\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    probs = calculate_class_priors(y_train)\n",
    "    densities = calculate_gaussian_density_params(X_train, y_train)\n",
    "    \n",
    "    ### Your code starts here\n",
    "    for x in X_test:\n",
    "\n",
    "        prob0 = probs[0]*np.product(np.array([gaussian_pdf(x[i], densities[0][i][0], densities[0][i][1]) for i in range(len(x))]))\n",
    "        prob1 = probs[1]*np.product(np.array([gaussian_pdf(x[i], densities[1][i][0], densities[1][i][1]) for i in range(len(x))]))\n",
    "        y = math.log(prob1 / prob0) > 0; \n",
    "        predictions.append(y)\n",
    "        \n",
    "    \n",
    "    ### End of your code\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = naive_bayes_classifier(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a test accuracy > 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 51.6%.\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_accuracy(predictions, y_test)\n",
    "print(f\"The test accuracy is {accuracy * 100}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4B: Image Completion with Mixture of Bernoullis and EM (70pt + 20 bonus pt )\n",
    "\n",
    "### Background\n",
    "\n",
    "#### Mixture of Bernoullis\n",
    "Each image we'll deal with is 28-by-28, and each pixel takes values in $\\{0, 1\\}$. For simplicity, we ignore the spatial structure of the images, and flatten the image into a 784-dimensional binary vectors.\n",
    "\n",
    "Each mixture of Bernoullis component consists of a collection of independent Bernoulli random variables. By denoting the image as $\\mathbf{x} \\in \\mathbb{R}^{784}$, and the latent variable $z$. Then the conditional probability of an image $\\mathbf{x} = \\mathbf{x}^{(i)}$ given the label ${z}=k$ is\n",
    "\\begin{align}\n",
    "    p(\\mathbf{x}^{(i)}|{z}=k)&= \\prod_{j=1}^{784} p(\\mathbf{x}_{j}^{(i)}|z=k) \\\\\n",
    "    & =  \\prod_{j=1}^{784} \\mathbf{\\theta}_{k, j}^{\\mathbf{x}_{j}^{(i)}} (1-\\mathbf{\\theta}_{k, j})^{1-\\mathbf{x}_{j}^{(i)}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "You should try to understand where this formula comes from. You'll find it useful when you do the derivations. This can be written out as the following generative process:\n",
    "\n",
    "1. Sample z from a multinomial distribution with parameter vector $\\mathbf{\\pi}$, where $\\mathbf{\\pi}$ is a $K$-dimensional vector in our case.\n",
    "\n",
    "2. For j = 1, ..., 784: Sample $\\mathbf{x}_j$ from a Bernoulli distribution with parameter $\\mathbf{\\theta}_{k,j}$, where $k$ is the value of $z$ from the first step.\n",
    "\n",
    "More formally, this can be written mathematically as:\n",
    "$$z \\sim \\text{Multinomial}(\\mathbf{\\pi})$$\n",
    "$$x_j|z=k \\sim \\text{Bernoulli}(\\theta_{k,j})$$\n",
    "\n",
    "\n",
    "\n",
    "#### Summary of notations\n",
    "\n",
    "The inputs (i.e., the training data) are represented by $\\mathbf{X}$, which is a $N$-by-$D$ binary matrix. In the E-step,we compute $\\mathbf{R}$, the matrix of membership weights, which is a $N$-by-$K$ matrix with $K$ being the number of components. Each row of $\\mathbf{R}$ gives the membership weights for one training example.\n",
    "\n",
    "The **trainable parameters** of the model, written out as vectors and matrices, are:\n",
    "\n",
    "$$\\mathbf{\\pi} = \\begin{bmatrix}\\pi_1 \\\\ \\pi_2 \\\\\\cdots \\\\ \\pi_K\\end{bmatrix}$$\n",
    "$$\\mathbf{\\Theta} = \\begin{bmatrix} \\theta_{1,1} & \\theta_{1,2} & \\ldots & \\theta_{1, N} \\\\ \\theta_{2,1} & \\theta_{2,2} & \\ldots & \\theta_{2, N} \\\\ \\cdots & & \\ddots & \\cdots \\\\ \\theta_{K,1} & \\theta_{K,2} & \\ldots & \\theta_{K, N}\\end{bmatrix}$$\n",
    "\n",
    "Since we are using the MNIST dataset, we have $N=60000$, and $D=784$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter code\n",
    "\n",
    "This code has been provided for you; there is no need to modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab\n",
    "import time\n",
    "\n",
    "# import checking\n",
    "import util\n",
    "\n",
    "try:\n",
    "    from scipy.special import gammaln\n",
    "except:\n",
    "    pass\n",
    "\n",
    "IMAGE_DIM = 28\n",
    "\n",
    "TRAIN_IMAGES_FILE = 'train-images-idx3-ubyte'\n",
    "TRAIN_LABELS_FILE = 'train-labels-idx1-ubyte'\n",
    "TEST_IMAGES_FILE = 't10k-images-idx3-ubyte'\n",
    "TEST_LABELS_FILE = 't10k-labels-idx1-ubyte'\n",
    "\n",
    "\n",
    "def beta_log_pdf(theta, a, b):\n",
    "    \"\"\"Log PDF of the beta distribution. We don't need this function, but we\n",
    "    include it in case you're interested. You need SciPy in order to use it.\"\"\"\n",
    "    norm_const = gammaln(a + b) - gammaln(a) - gammaln(b)\n",
    "    return norm_const + (a - 1.) * np.log(theta) + (b - 1.) * np.log(1. - theta)\n",
    "\n",
    "def beta_log_pdf_unnorm(theta, a, b):\n",
    "    \"\"\"Unnormalized log PDF of the beta distribution.\"\"\"\n",
    "    return (a - 1.) * np.log(theta) + (b - 1.) * np.log(1. - theta)\n",
    "\n",
    "def dirichlet_log_pdf(pi, a):\n",
    "    \"\"\"Log PDF of the Dirichlet distribution. We don't need this function, but we\n",
    "    include it in case you're interested. You need SciPy in order to use it.\"\"\"\n",
    "    norm_const = gammaln(a.sum()) - gammaln(a).sum()\n",
    "    return norm_const + np.sum((a - 1.) * np.log(pi))\n",
    "\n",
    "def dirichlet_log_pdf_unnorm(pi, a):\n",
    "    \"\"\"Unnormalized log PDF of the Dirichlet distribution.\"\"\"\n",
    "    return np.sum((a - 1.) * np.log(pi))\n",
    "\n",
    "\n",
    "class Params(object):\n",
    "    \"\"\"A class which represents the trainable parameters of the mixture model.\n",
    "        - pi: the mixing proportions, represented as a K-dimensional array. It must be a\n",
    "            probability distribution, i.e. the entries must be nonnegative and sum to 1.\n",
    "        - theta: The Bernoulli parameters for each pixel in each mixture component. This is\n",
    "            a K x D matrix, where rows correspond to mixture components and columns correspond\n",
    "            to pixels. \"\"\"\n",
    "\n",
    "    def __init__(self, pi, theta):\n",
    "        self.pi = pi\n",
    "        self.theta = theta\n",
    "\n",
    "    @classmethod\n",
    "    def random_initialization(cls, num_components, num_pixels):\n",
    "        init_pi = np.ones(num_components) / num_components\n",
    "        init_theta = np.random.uniform(0.49, 0.51, size=(num_components, num_pixels))\n",
    "        return Params(init_pi, init_theta)\n",
    "\n",
    "class Prior(object):\n",
    "    \"\"\"A class representing the priors over parameters in the mixture model.\n",
    "        - a_mix: A scalar valued parameter for the Dirichlet prior over mixing proportions.\n",
    "        - a_pixels and b_pixels: The scalar-valued parameters for the beta prior over the entries of\n",
    "            theta. I.e., the entries of theta are assumed to be drawn i.i.d. from the distribution\n",
    "            Beta(a_pixels, b_pixels). \"\"\"\n",
    "\n",
    "    def __init__(self, a_mix, a_pixels, b_pixels):\n",
    "        self.a_mix = a_mix\n",
    "        self.a_pixels = a_pixels\n",
    "        self.b_pixels = b_pixels\n",
    "\n",
    "    @classmethod\n",
    "    def default_prior(cls):\n",
    "        \"\"\"Return a Prior instance which has reasonable values.\"\"\"\n",
    "        return cls(2., 2., 2.)\n",
    "\n",
    "    @classmethod\n",
    "    def uniform_prior(cls):\n",
    "        \"\"\"Return a set of prior parameters which corresponds to a uniform distribution. Then\n",
    "        MAP estimation is equivalent to maximum likelihood.\"\"\"\n",
    "        return cls(1., 1., 1.)\n",
    "\n",
    "\n",
    "def multinomial_entropy(p):\n",
    "    \"\"\"Compute the entropy of a Bernoulli random variable, in nats rather than bits.\"\"\"\n",
    "    p = np.clip(p, 1e-20, np.infty)      # avoid taking the log of 0\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "\n",
    "def variational_objective(model, X, R, pi, theta):\n",
    "    \"\"\"Compute the variational lower bound on the log-likelihood that each step of E-M\n",
    "    is maximizing. This is described in the paper\n",
    "\n",
    "        Neal and Hinton, 1998. A view of the E-M algorithm that justifies incremental, sparse, and other variants.\n",
    "\n",
    "    We can test the update rules by verifying that each step maximizes this bound.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Model(model.prior, Params(pi, theta))\n",
    "    expected_log_prob = model.expected_joint_log_probability(X, R)\n",
    "    entropy_term = np.sum(multinomial_entropy(R))\n",
    "    return expected_log_prob + entropy_term\n",
    "\n",
    "def perturb_pi(pi, eps=1e-6):\n",
    "    pi = np.random.normal(pi, eps)\n",
    "    pi = np.clip(pi, 1e-10, np.infty)\n",
    "    pi /= pi.sum()\n",
    "    return pi\n",
    "\n",
    "def perturb_theta(theta, eps=1e-6):\n",
    "    theta = np.random.normal(theta, eps)\n",
    "    theta = np.clip(theta, 1e-10, 1. - 1e-10)\n",
    "    return theta\n",
    "\n",
    "def perturb_R(R, eps=1e-6):\n",
    "    R = np.random.normal(R, eps)\n",
    "    R = np.clip(R, 1e-10, np.infty)\n",
    "    R /= R.sum(1).reshape((-1, 1))\n",
    "    return R\n",
    "\n",
    "def train_from_labels(prior=None, show=True):\n",
    "    \"\"\"Fit the mixture model using the labeled MNIST data. There are 10 mixture components,\n",
    "    one corresponding to each of the digit classes.\"\"\"\n",
    "\n",
    "    X = util.read_mnist_images(TRAIN_IMAGES_FILE)\n",
    "    y = util.read_mnist_labels(TRAIN_LABELS_FILE)\n",
    "    X_test = util.read_mnist_images(TEST_IMAGES_FILE)\n",
    "    num_data, num_pixels = X.shape\n",
    "\n",
    "    if prior is None:\n",
    "        prior = Prior.default_prior()\n",
    "    model = Model.random_initialization(prior, 10, IMAGE_DIM**2)\n",
    "\n",
    "    R = np.zeros((num_data, 10))\n",
    "    R[np.arange(num_data), y] = 1.\n",
    "    model.params.pi = model.update_pi(R)\n",
    "    model.params.theta = model.update_theta(X, R)\n",
    "\n",
    "    # mask which includes top half of pixels\n",
    "    M = np.zeros(X.shape, dtype=int)\n",
    "    M[:, :M.shape[1]//2] = 1\n",
    "\n",
    "    if show:\n",
    "        model.visualize_components()\n",
    "        try:\n",
    "            model.visualize_predictions(X[:64, :], M[:64, :])\n",
    "        except:\n",
    "            print('Posterior predictive distribution not implemented yet.')\n",
    "\n",
    "        print('Training log-likelihood:', model.log_likelihood(X) / num_data)\n",
    "        print('Test log-likelihood:', model.log_likelihood(X_test) / X_test.shape[0])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4B-I: Parameter Learning via EM (35pt)\n",
    "\n",
    "In the first step, we’ll learn the parameters of the model given the responsibilities, using the Maximum A Posteriori (MAP) criterion. This corresponds to the M-step of the E-M (Expectation-Maximization) algorithm.\n",
    "\n",
    "In lecture, we discussed the E-M algorithm in the context of maximum likelihood (ML) learning. This is discussed in detail in slides of Lecture 18 Gaussian Mixture Model, and **you should read those slides carefully before starting this part**.\n",
    "\n",
    "The MAP case is only slightly different from ML: the only difference is that we add a prior probability term to the objective function in the M-step.\n",
    "In particular, recall that in the context of ML, the M-step maximizes the objective function:\n",
    "\n",
    "$$\\sum_{i=1}^N \\sum_{k=1}^K r^{(i)}_{k} \\left[\\log p(z^{(i)}=k) + \\log p(x^{(i)}|z^{(i)}=k)\\right]$$\n",
    "\n",
    "where the $r^{(i)}_k$ are the membership weights computed during the E-step. In the MAP formulation, we need to incorporate\n",
    "information about our prior distribution, i.e. we add the (log) prior probability of the parameters:\n",
    "\n",
    "$$\\sum_{i=1}^N \\sum_{k=1}^K r^{(i)}_{k} \\left[\\log p(z^{(i)}=k) + \\log p(x^{(i)}|z^{(i)}=k)\\right] + \\log p(\\mathbf{\\pi}) + \\log p(\\mathbf{\\Theta})$$\n",
    "\n",
    "Our prior for $\\mathbf{\\Theta}$ is as follows: every entry is drawn independently from a [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) with parameters a and b. The beta distribution is\n",
    "\n",
    "$$p(\\theta_{k,j})\\propto \\theta_{k,j}^{a-1}(1-\\theta_{k,j})^{b-1}$$\n",
    "\n",
    "Recall that $\\propto$ means “proportional to” i.e., the distribution has a normalizing constant to ensure it integrates to 1 which we’re ignoring because we don’t need it for the M-step.\n",
    "\n",
    "For the prior over mixing proportions $\\mathbf{\\pi}$, we’ll use the [Dirichlet distribution](http://en.wikipedia.org/wiki/Dirichlet_distribution), which is the conjugate prior for the multinomial distribution. It is a distribution over the probability simplex, i.e.the set of vectors which define a valid probability distribution. The distribution takes the form:\n",
    "\n",
    "$$p(\\mathbf{\\pi})\\propto \\pi_1^{a_1-1}\\pi_2^{a_2-1}\\ldots\\pi_{K}^{a_K-1}$$\n",
    "\n",
    "For simplicity, we use a symmetric Dirichlet prior where all the $a_k$ parameters are assumed to be **equal**. Like the beta distribution, the Dirichlet distribution has a normalizing constant which we don’t need when updating the parameters. The beta distribution is actually the special case of the Dirichlet distribution for $K = 2$.\n",
    "\n",
    "Further reading on the EM algorithm can be found in Section 8.7 of Murphy's Probabilistic Machine Learning (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Derive the M-step update rules\n",
    "Derive the M-step update rules for $\\mathbf{\\Theta}$ and $\\mathbf{\\pi}$ by setting the partial derivatives of the following equation to be zero:\n",
    "\n",
    "$$\\sum_{i=1}^N \\sum_{k=1}^K r^{(i)}_{k} \\left[\\log p(z^{(i)}=k) + \\log p(x^{(i)}|z^{(i)}=k)\\right] + \\log p(\\mathbf{\\pi}) + \\log p(\\mathbf{\\Theta})$$\n",
    "\n",
    "**Implementation**: Take these formulas and use them to implement the functions `Model.update_pi` and `Model.update_theta` below. Each one should be implemented in terms of NumPy matrix and vector operations. Each one requires only a few lines of code, and should not involve any for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"A class implementing the mixture of Bernoullis model. The fields are:\n",
    "        - prior: an Prior instance\n",
    "        - params: a Params instance\"\"\"\n",
    "\n",
    "    def __init__(self, prior, params):\n",
    "        self.prior = prior\n",
    "        self.params = params\n",
    "\n",
    "    @classmethod\n",
    "    def random_initialization(cls, prior, num_components, num_pixels):\n",
    "        params = Params.random_initialization(num_components, num_pixels)\n",
    "        return cls(prior, params)\n",
    "\n",
    "    def expected_joint_log_probability(self, X, R):\n",
    "        \"\"\"Compute the expected joint log probability, where the expectation is with respect to\n",
    "        the responsibilities R. This is the objective function being maximized in the M-step.\n",
    "        It's useful for verifying the optimality conditions in the M-step.\"\"\"\n",
    "\n",
    "        total = 0.\n",
    "\n",
    "        # Prior over mixing proportions\n",
    "        total += dirichlet_log_pdf_unnorm(self.params.pi, self.prior.a_mix)\n",
    "\n",
    "        # Prior over pixel probabilities\n",
    "        total += np.sum(beta_log_pdf_unnorm(self.params.theta, self.prior.a_pixels, self.prior.b_pixels))\n",
    "\n",
    "        # Probability of assignments\n",
    "        total += np.sum(R * np.log(self.params.pi))\n",
    "\n",
    "        # Matrix of log probabilities of observations conditioned on z\n",
    "        # The (i, k) entry is p(x^(i) | z^(i) = k)\n",
    "        log_p_x_given_z = np.dot(X, np.log(self.params.theta).T) + \\\n",
    "                          np.dot(1. - X, np.log(1. - self.params.theta).T)\n",
    "\n",
    "        # Observation probabilities\n",
    "        total += np.sum(R * log_p_x_given_z)\n",
    "\n",
    "        return total\n",
    "\n",
    "    def log_likelihood(self, X):\n",
    "        \"\"\"Compute the log-likelihood of the observed data, i.e. the log probability with the\n",
    "        latent variables marginalized out.\"\"\"\n",
    "\n",
    "        # Matrix of log probabilities of observations conditioned on z\n",
    "        # The (i, k) entry is p(x^(i) | z^(i) = k)\n",
    "        log_p_x_given_z = np.dot(X, np.log(self.params.theta).T) + \\\n",
    "                          np.dot(1. - X, np.log(1. - self.params.theta).T)\n",
    "        log_p_z_x = log_p_x_given_z + np.log(self.params.pi)\n",
    "\n",
    "        # This is a numerically stable way to compute np.log(np.sum(np.exp(log_p_z_x), axis=1))\n",
    "        log_p_x = np.logaddexp.reduce(log_p_z_x, axis=1)\n",
    "\n",
    "        return log_p_x.sum()\n",
    "\n",
    "    def update_pi(self, R):\n",
    "        \"\"\"Compute the update for the mixing proportions in the M-step of the E-M algorithm.\n",
    "        You should derive the optimal value of pi (the one which maximizes the expected log\n",
    "        probability) by setting the partial derivatives of the Lagrangian to zero. You should\n",
    "        implement this in terms of NumPy matrix and vector operations, rather than a for loop.\"\"\"\n",
    "\n",
    "        ######################## Your code here #########################\n",
    "\n",
    "        #################################################################\n",
    "\n",
    "    def update_theta(self, X, R):\n",
    "        \"\"\"Compute the update for the Bernoulli parameters in the M-step of the E-M algorithm.\n",
    "        You should derive the optimal value of theta (the one which maximizes the expected log\n",
    "        probability) by setting the partial derivatives to zero. You should implement this in\n",
    "        terms of NumPy matrix and vector operations, rather than a for loop.\"\"\"\n",
    "\n",
    "        ######################## Your code here #########################\n",
    "\n",
    "        #################################################################\n",
    "\n",
    "    def compute_posterior(self, X, M=None):\n",
    "        \"\"\"Compute the posterior probabilities of the cluster assignments given the observations.\n",
    "        This is used to compute the E-step of the E-M algorithm. It's also used in computing the\n",
    "        posterior predictive distribution when making inferences about the hidden part of the image.\n",
    "        It takes an optional parameter M, which is a binary matrix the same size as X, and determines\n",
    "        which pixels are observed. (1 means observed, and 0 means unobserved.)\n",
    "        Your job is to compute the variable log_p_z_x, which is a matrix whose (i, k) entry is the\n",
    "        log of the joint proability, i.e.\n",
    "             log p(z^(i) = k, x^(i)) = log p(z^(i) = k) + log p(x^(i) | z^(i) = k)\n",
    "        Hint: the solution is a small modification of the computation of log_p_z_x in\n",
    "        Model.log_likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        if M is None:\n",
    "            M = np.ones(X.shape, dtype=int)\n",
    "\n",
    "        ######################## Your code here #########################\n",
    "\n",
    "        #################################################################\n",
    "\n",
    "        # subtract the max of each row to avoid numerical instability\n",
    "        log_p_z_x_shifted = log_p_z_x - log_p_z_x.max(1).reshape((-1, 1))\n",
    "\n",
    "        # convert the log probabilities to probabilities and renormalize\n",
    "        R = np.exp(log_p_z_x_shifted)\n",
    "        R /= R.sum(1).reshape((-1, 1))\n",
    "        return R\n",
    "\n",
    "    def posterior_predictive_means(self, X, M):\n",
    "        \"\"\"Compute the matrix of posterior predictive means for unobserved pixels given the observed\n",
    "        pixels. The matrix M is a binary matrix the same size as X which determines which pixels\n",
    "        are observed. (1 means observed, and 0 means unobserved.) You should return a real-valued\n",
    "        matrix the same size as X. For all the entries corresponding to unobserved pixels, the value\n",
    "        should determine the posterior probability that the pixel is on, conditioned on the observed\n",
    "        pixels. It does not matter what values you assign for observed pixels, since those values\n",
    "        aren't used for anything. \"\"\"\n",
    "        return np.dot(self.compute_posterior(X, M), self.params.theta)\n",
    "\n",
    "    def visualize_components(self, title=None):\n",
    "        \"\"\"Visualize the learned components. Each of the images shows the Bernoulli parameters\n",
    "        (probability of the pixel being 1) for one of the mixture components.\"\"\"\n",
    "\n",
    "        pylab.figure('Mixture components')\n",
    "        pylab.matshow(util.arrange(self.params.theta.reshape((-1, IMAGE_DIM, IMAGE_DIM))),\n",
    "                      fignum=False, cmap='gray')\n",
    "        if title is None:\n",
    "            title = 'Mixture components'\n",
    "        pylab.title(title)\n",
    "        pylab.draw()\n",
    "\n",
    "    def visualize_predictions(self, X, M, title=None):\n",
    "        \"\"\"Visualize the predicted probabilities for each of the missing pixels.\"\"\"\n",
    "\n",
    "        P = self.posterior_predictive_means(X, M)\n",
    "        imgs = np.where(M, X, P)\n",
    "        obs = np.where(M, X, 0.3)\n",
    "\n",
    "        pylab.figure('Observations')\n",
    "        pylab.matshow(util.arrange(obs.reshape((-1, IMAGE_DIM, IMAGE_DIM))),\n",
    "                      fignum=False, cmap='gray')\n",
    "        pylab.title('Observations')\n",
    "\n",
    "        pylab.figure('Model predictions')\n",
    "        pylab.matshow(util.arrange(imgs.reshape((-1, IMAGE_DIM, IMAGE_DIM))),\n",
    "                      fignum=False, cmap='gray')\n",
    "        if title is None:\n",
    "            title = 'Model predictions'\n",
    "        pylab.title(title)\n",
    "        pylab.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking**: Once you have finished the implementation, please run `check_m_step()` for a sanity check. If you pass this sanity check, your implementation is probably correct. (You **DON'T** need to understand how the code works.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_m_step():\n",
    "    \"\"\"Check that the M-step updates by making sure they maximize the variational\n",
    "    objective with respect to the model parameters.\"\"\"\n",
    "    np.random.seed(0)\n",
    "\n",
    "    NUM_IMAGES = 100\n",
    "\n",
    "    X = util.read_mnist_images(TRAIN_IMAGES_FILE)\n",
    "    X = X[:NUM_IMAGES, :]\n",
    "    R = np.random.uniform(size=(NUM_IMAGES, 10))\n",
    "    R /= R.sum(1).reshape((-1, 1))\n",
    "    model = Model.random_initialization(Prior.default_prior(), 10, 784)\n",
    "\n",
    "    theta = model.update_theta(X, R)\n",
    "    pi = model.update_pi(R)\n",
    "\n",
    "\n",
    "\n",
    "    opt = variational_objective(model, X, R, pi, theta)\n",
    "\n",
    "    ok = True\n",
    "    for i in range(20):\n",
    "        new_theta = perturb_theta(theta)\n",
    "        new_obj = variational_objective(model, X, R, pi, new_theta)\n",
    "        if new_obj > opt:\n",
    "            ok = False\n",
    "    if ok:\n",
    "        print('The theta update seems OK.')\n",
    "    else:\n",
    "        print('Something seems to be wrong with the theta update.')\n",
    "\n",
    "    if not np.allclose(np.sum(pi), 1.):\n",
    "        print('Uh-oh. pi does not seem to sum to 1.')\n",
    "    else:\n",
    "        ok = True\n",
    "        for i in range(20):\n",
    "            new_pi = perturb_pi(pi)\n",
    "            new_obj = variational_objective(model, X, R, new_pi, theta)\n",
    "            if new_obj > opt:\n",
    "                ok = False\n",
    "        if ok:\n",
    "            print('The pi update seems OK.')\n",
    "        else:\n",
    "            print('Something seems to be wrong with the pi update.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type NoneType which has no callable log method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'log'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheck_m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 19\u001b[0m, in \u001b[0;36mcheck_m_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m theta \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mupdate_theta(X, R)\n\u001b[1;32m     15\u001b[0m pi \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mupdate_pi(R)\n\u001b[0;32m---> 19\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mvariational_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n",
      "Cell \u001b[0;32mIn[50], line 100\u001b[0m, in \u001b[0;36mvariational_objective\u001b[0;34m(model, X, R, pi, theta)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the variational lower bound on the log-likelihood that each step of E-M\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03mis maximizing. This is described in the paper\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03mWe can test the update rules by verifying that each step maximizes this bound.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(model\u001b[38;5;241m.\u001b[39mprior, Params(pi, theta))\n\u001b[0;32m--> 100\u001b[0m expected_log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpected_joint_log_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m entropy_term \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(multinomial_entropy(R))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m expected_log_prob \u001b[38;5;241m+\u001b[39m entropy_term\n",
      "Cell \u001b[0;32mIn[51], line 23\u001b[0m, in \u001b[0;36mModel.expected_joint_log_probability\u001b[0;34m(self, X, R)\u001b[0m\n\u001b[1;32m     20\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Prior over mixing proportions\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mdirichlet_log_pdf_unnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma_mix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Prior over pixel probabilities\u001b[39;00m\n\u001b[1;32m     26\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(beta_log_pdf_unnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mtheta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior\u001b[38;5;241m.\u001b[39ma_pixels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior\u001b[38;5;241m.\u001b[39mb_pixels))\n",
      "Cell \u001b[0;32mIn[50], line 39\u001b[0m, in \u001b[0;36mdirichlet_log_pdf_unnorm\u001b[0;34m(pi, a)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdirichlet_log_pdf_unnorm\u001b[39m(pi, a):\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Unnormalized log PDF of the Dirichlet distribution.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum((a \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type NoneType which has no callable log method"
     ]
    }
   ],
   "source": [
    "check_m_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4B-II: Posterior inference (30pt)\n",
    "Now we derive the posterior probability distribution $p(z|\\mathbf{x}_{\\text{obs}})$, where $\\mathbf{x}_{\\text{obs}}$ denotes the subset of the\n",
    "pixels which are observed. In the implementation, we will represent partial observations in terms\n",
    "of variables $m^{(i)}_j$, where $m^{(i)}_j=1$ if the $j$th pixel of the $i$th image is observed, and $0$ otherwise. In the implementation, we organize the $m_j^{(i)}$'s into a matrix $\\mathbf{M}$ which is the same shape as $\\mathbf{X}$.\n",
    "\n",
    "#### Question: Computing the posterior\n",
    "\n",
    "Derive the rule for computing the posterior probability distribution $p(z|\\mathbf{x})$. Your final answer should look something like,\n",
    "\n",
    "$$p(z=k|\\mathbf{x})= \\ldots$$\n",
    "\n",
    "where the ellipsis represents something you could actually implement. Note that the image\n",
    "may be only partially observed.\n",
    "\n",
    "**Implementation**: Implement the method `Model.compute_posterior` using your solution to the previous question. While your answer to Question 1 was probably given in terms of probabilities, we do the computations in terms of log probabilities for numerical stability. We’ve already filled in part of the implementation, so your job is to compute $\\log p(z, \\mathbf{x})$ as described in the method’s doc string.\n",
    "\n",
    "Your implementation should use NumPy matrix and vector operations, rather than a for loop. *Hint: There are two lines in `Model.log_likelihood` which are almost a solution to this question. You can reuse these lines as part of the solution, except you’ll need to modify them to deal with partial observations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_e_step():\n",
    "    \"\"\"Check the E-step updates by making sure they maximize the variational\n",
    "    objective with respect to the responsibilities. Note that this does not\n",
    "    fully check your solution to Part 2, since it only applies to fully observed\n",
    "    images.\"\"\"\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    NUM_IMAGES = 100\n",
    "\n",
    "    X = util.read_mnist_images(TRAIN_IMAGES_FILE)\n",
    "    X = X[:NUM_IMAGES, :]\n",
    "    model = train_from_labels(show=False)\n",
    "\n",
    "    # reduce the number of observations so that the posterior is less peaked\n",
    "    X = X[:, ::50]\n",
    "    model.params.theta = model.params.theta[:, ::50]\n",
    "\n",
    "    R = model.compute_posterior(X)\n",
    "\n",
    "    opt = variational_objective(model, X, R, model.params.pi, model.params.theta)\n",
    "\n",
    "    if not np.allclose(R.sum(1), 1.):\n",
    "        print('Uh-oh. Rows of R do not seem to sum to 1.')\n",
    "    else:\n",
    "        ok = True\n",
    "        for i in range(20):\n",
    "            new_R = perturb_R(R)\n",
    "            new_obj = variational_objective(model, X, new_R, model.params.pi, model.params.theta)\n",
    "            if new_obj > opt:\n",
    "                ok = False\n",
    "        if ok:\n",
    "            print('The E-step seems OK.')\n",
    "        else:\n",
    "            print('Something seems to be wrong with the E-step.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking**: To help you check your solution, we’ve provided the function `check_e_step()`. Note that this check only covers the case where the image is fully observed, so it doesn’t fully verify your solution to this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_e_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4B-III: Report the results (5pt)\n",
    "\n",
    "Run the following code  to generate the results and compile them in the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_em(num_components=100, num_steps=50, prior=None, draw_every=1):\n",
    "    \"\"\"Fit the mixture model in an unsupervised fashion using E-M.\"\"\"\n",
    "\n",
    "    X = util.read_mnist_images(TRAIN_IMAGES_FILE)\n",
    "    X_test = util.read_mnist_images(TEST_IMAGES_FILE)\n",
    "    num_data, num_pixels = X.shape\n",
    "\n",
    "    if prior is None:\n",
    "        prior = Prior.default_prior()\n",
    "    model = Model.random_initialization(prior, num_components, num_pixels)\n",
    "\n",
    "    # mask which includes top half of pixels\n",
    "    M = np.zeros(X.shape, dtype=int)\n",
    "    M[:, :M.shape[1]//2] = 1\n",
    "\n",
    "    loglik_vals = []\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        # E-step\n",
    "        R = model.compute_posterior(X)\n",
    "\n",
    "        # M-step\n",
    "        model.params.pi = model.update_pi(R)\n",
    "        model.params.theta = model.update_theta(X, R)\n",
    "\n",
    "        loglik = model.log_likelihood(X) / num_data\n",
    "        loglik_vals.append(loglik)\n",
    "\n",
    "        if (i+1) % draw_every == 0:\n",
    "            model.visualize_components()\n",
    "            model.visualize_predictions(X[:64, :], M[:64, :])\n",
    "\n",
    "            pylab.figure('Log-likelihood')\n",
    "            pylab.clf()\n",
    "            pylab.semilogx(np.arange(1, i+2), loglik_vals)\n",
    "            pylab.title('Log-likelihood')\n",
    "            pylab.xlabel('Number of E-M steps')\n",
    "            pylab.draw()\n",
    "\n",
    "    print('Final training log-likelihood:', model.log_likelihood(X) / num_data)\n",
    "    print('Final test log-likelihood:', model.log_likelihood(X_test) / X_test.shape[0])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take several minutes (<10 minutes) to finish.\n",
    "model = train_with_em(num_components=100, num_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus (20 Pt): Task 4B-IV: Anomaly detection\n",
    "\n",
    "Please implement an anomaly detection algorithm for the provided dataset, which includes one outlier. Your implementation should be able to detect the outlier based on previous methods and also visualize the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = util.read_mnist_images(TEST_IMAGES_FILE)\n",
    "outlier = np.random.randn(1, 784)\n",
    "X_test = np.vstack([X_test, outlier])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
